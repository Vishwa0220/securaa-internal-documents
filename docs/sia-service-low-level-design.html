<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Securaa Platform Documentation - SIA Service - Low Level Design">
    <title>SIA Service - Low Level Design - Securaa Documentation</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <style>

:root {
    --primary-color: #4f46e5;
    --primary-dark: #3730a3;
    --primary-light: #818cf8;
    --secondary-color: #06b6d4;
    --accent-color: #f59e0b;
    --success-color: #10b981;
    --warning-color: #f59e0b;
    --error-color: #ef4444;
    --text-primary: #1f2937;
    --text-secondary: #6b7280;
    --text-muted: #9ca3af;
    --bg-primary: #ffffff;
    --bg-secondary: #f9fafb;
    --bg-tertiary: #f3f4f6;
    --border-color: #e5e7eb;
    --shadow-sm: 0 1px 2px 0 rgba(0, 0, 0, 0.05);
    --shadow-md: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
    --shadow-lg: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
}

* {
    margin: 0;
    padding: 0;
    box-sizing: border-box;
}

html {
    scroll-behavior: smooth;
    font-size: 16px;
}

body {
    font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
    line-height: 1.7;
    color: var(--text-primary);
    background-color: var(--bg-secondary);
}

/* Header */
.main-header {
    background: linear-gradient(135deg, var(--primary-color) 0%, var(--primary-dark) 100%);
    color: white;
    padding: 1rem 2rem;
    position: sticky;
    top: 0;
    z-index: 1000;
    box-shadow: var(--shadow-lg);
}

.header-content {
    max-width: 1400px;
    margin: 0 auto;
    display: flex;
    justify-content: space-between;
    align-items: center;
}

.header-logo {
    font-size: 1.5rem;
    font-weight: 700;
    letter-spacing: -0.025em;
}

.header-logo span {
    color: var(--secondary-color);
}

/* Navigation */
.documentation-nav {
    background: var(--bg-primary);
    border-bottom: 1px solid var(--border-color);
    padding: 0.75rem 2rem;
    position: sticky;
    top: 60px;
    z-index: 999;
    box-shadow: var(--shadow-sm);
}

.nav-links {
    max-width: 1400px;
    margin: 0 auto;
    display: flex;
    gap: 1.5rem;
    flex-wrap: wrap;
    justify-content: center;
}

.nav-links a {
    color: var(--text-secondary);
    text-decoration: none;
    font-size: 0.875rem;
    font-weight: 500;
    padding: 0.5rem 0.75rem;
    border-radius: 0.375rem;
    transition: all 0.2s ease;
}

.nav-links a:hover {
    color: var(--primary-color);
    background: var(--bg-tertiary);
}

/* Main Content */
.main-content {
    max-width: 1000px;
    margin: 2rem auto;
    padding: 2.5rem;
    background: var(--bg-primary);
    border-radius: 1rem;
    box-shadow: var(--shadow-md);
}

/* Typography */
h1, h2, h3, h4, h5, h6 {
    color: var(--text-primary);
    font-weight: 700;
    line-height: 1.3;
    margin-top: 2rem;
    margin-bottom: 1rem;
}

h1 {
    font-size: 2.5rem;
    background: linear-gradient(135deg, var(--primary-color), var(--primary-dark));
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    background-clip: text;
    padding-bottom: 0.75rem;
    border-bottom: 3px solid var(--primary-color);
    margin-top: 0;
}

h2 {
    font-size: 1.875rem;
    color: var(--primary-dark);
    border-bottom: 2px solid var(--border-color);
    padding-bottom: 0.5rem;
}

h3 {
    font-size: 1.5rem;
    color: var(--text-primary);
}

h4 {
    font-size: 1.25rem;
    color: var(--text-secondary);
}

h5 {
    font-size: 1.125rem;
}

h6 {
    font-size: 1rem;
    color: var(--text-muted);
}

p {
    margin-bottom: 1rem;
    color: var(--text-primary);
}

/* Links */
a {
    color: var(--primary-color);
    text-decoration: none;
    transition: color 0.2s ease;
}

a:hover {
    color: var(--primary-dark);
    text-decoration: underline;
}

/* Lists */
ul, ol {
    margin-bottom: 1rem;
    padding-left: 1.5rem;
}

li {
    margin-bottom: 0.5rem;
}

li > ul, li > ol {
    margin-top: 0.5rem;
    margin-bottom: 0;
}

/* Code Blocks */
pre {
    background: #1e1e1e;
    color: #d4d4d4;
    padding: 1.25rem;
    border-radius: 0.5rem;
    overflow-x: auto;
    margin: 1.5rem 0;
    font-size: 0.875rem;
    line-height: 1.6;
    box-shadow: var(--shadow-md);
}

code {
    font-family: 'JetBrains Mono', 'Fira Code', 'Consolas', monospace;
    font-size: 0.875em;
}

:not(pre) > code {
    background: var(--bg-tertiary);
    color: var(--primary-dark);
    padding: 0.2rem 0.4rem;
    border-radius: 0.25rem;
    font-size: 0.875em;
}

/* Tables */
table {
    width: 100%;
    border-collapse: collapse;
    margin: 1.5rem 0;
    font-size: 0.9rem;
    box-shadow: var(--shadow-sm);
    border-radius: 0.5rem;
    overflow: hidden;
}

thead {
    background: linear-gradient(135deg, var(--primary-color), var(--primary-dark));
    color: white;
}

th {
    padding: 1rem;
    text-align: left;
    font-weight: 600;
    text-transform: uppercase;
    font-size: 0.75rem;
    letter-spacing: 0.05em;
}

td {
    padding: 0.875rem 1rem;
    border-bottom: 1px solid var(--border-color);
}

tr:nth-child(even) {
    background: var(--bg-secondary);
}

tr:hover {
    background: var(--bg-tertiary);
}

/* Blockquotes */
blockquote {
    border-left: 4px solid var(--primary-color);
    padding: 1rem 1.5rem;
    margin: 1.5rem 0;
    background: var(--bg-secondary);
    border-radius: 0 0.5rem 0.5rem 0;
    font-style: italic;
    color: var(--text-secondary);
}

blockquote p:last-child {
    margin-bottom: 0;
}

/* Mermaid Diagrams - Enhanced Styling */
.mermaid {
    display: flex;
    justify-content: center;
    align-items: center;
    margin: 2rem 0;
    padding: 1.5rem;
    background: linear-gradient(135deg, #fafbfc 0%, #f0f4f8 100%);
    border-radius: 0.75rem;
    border: 1px solid var(--border-color);
    box-shadow: var(--shadow-sm);
    overflow-x: auto;
    overflow-y: visible;
    min-height: 200px;
}

.mermaid svg {
    max-width: 100%;
    height: auto;
    display: block;
    margin: 0 auto;
}

/* Ensure diagrams scale properly */
.mermaid[data-processed="true"] {
    min-height: auto;
}

/* Diagram container for better control */
.diagram-container {
    width: 100%;
    overflow-x: auto;
    padding: 1rem 0;
}

/* HR Styling */
hr {
    border: none;
    border-top: 2px solid var(--border-color);
    margin: 2.5rem 0;
}

/* Table of Contents */
.toc {
    background: var(--bg-secondary);
    padding: 1.5rem;
    border-radius: 0.5rem;
    margin-bottom: 2rem;
    border: 1px solid var(--border-color);
}

.toc-title {
    font-weight: 700;
    color: var(--primary-color);
    margin-bottom: 1rem;
    font-size: 1.125rem;
}

.toc ul {
    list-style: none;
    padding-left: 0;
}

.toc li {
    margin-bottom: 0.5rem;
}

.toc a {
    color: var(--text-secondary);
    font-size: 0.9rem;
}

.toc a:hover {
    color: var(--primary-color);
}

/* Document Info Box */
.doc-info {
    background: linear-gradient(135deg, var(--bg-secondary), var(--bg-tertiary));
    border: 1px solid var(--border-color);
    border-radius: 0.5rem;
    padding: 1rem 1.5rem;
    margin-bottom: 2rem;
    font-size: 0.875rem;
}

.doc-info strong {
    color: var(--primary-color);
}

/* Footer */
.footer {
    text-align: center;
    padding: 2rem;
    background: var(--bg-tertiary);
    color: var(--text-muted);
    font-size: 0.875rem;
    margin-top: 2rem;
}

/* Scrollbar Styling */
::-webkit-scrollbar {
    width: 8px;
    height: 8px;
}

::-webkit-scrollbar-track {
    background: var(--bg-tertiary);
}

::-webkit-scrollbar-thumb {
    background: var(--text-muted);
    border-radius: 4px;
}

::-webkit-scrollbar-thumb:hover {
    background: var(--text-secondary);
}

/* Print Styles */
@media print {
    .main-header, .documentation-nav, .footer {
        display: none !important;
    }

    .main-content {
        max-width: none;
        margin: 0;
        padding: 20px;
        box-shadow: none;
        border-radius: 0;
    }

    body {
        background: white;
        font-size: 11pt;
    }

    h1 {
        font-size: 24pt;
        -webkit-text-fill-color: var(--primary-dark);
        page-break-after: avoid;
    }

    h2, h3, h4 {
        page-break-after: avoid;
    }

    pre {
        background: #f5f5f5 !important;
        color: #333 !important;
        border: 1px solid #ddd;
        page-break-inside: avoid;
        font-size: 9pt;
    }

    table {
        page-break-inside: avoid;
    }

    .mermaid {
        page-break-inside: avoid;
        background: white !important;
        border: 1px solid #ddd;
        max-width: 100% !important;
    }

    .mermaid svg {
        max-width: 100% !important;
        max-height: 700px !important;
    }

    a {
        color: var(--primary-dark) !important;
        text-decoration: none !important;
    }
}

/* Responsive Design */
@media (max-width: 768px) {
    .main-content {
        margin: 1rem;
        padding: 1.5rem;
        border-radius: 0.5rem;
    }

    h1 {
        font-size: 1.75rem;
    }

    h2 {
        font-size: 1.5rem;
    }

    .nav-links {
        gap: 0.75rem;
    }

    .nav-links a {
        font-size: 0.75rem;
        padding: 0.375rem 0.5rem;
    }

    pre {
        font-size: 0.8rem;
        padding: 1rem;
    }

    table {
        font-size: 0.8rem;
    }

    th, td {
        padding: 0.5rem;
    }
}

/* Animation for smooth transitions */
@keyframes fadeIn {
    from { opacity: 0; transform: translateY(10px); }
    to { opacity: 1; transform: translateY(0); }
}

.main-content {
    animation: fadeIn 0.3s ease-out;
}

/* Index-specific styles */
.hero {
    text-align: center;
    padding: 3rem 0;
    background: linear-gradient(135deg, var(--bg-secondary), var(--bg-tertiary));
    border-radius: 1rem;
    margin-bottom: 3rem;
}

.hero h1 {
    font-size: 3rem;
    margin-bottom: 1rem;
    border: none;
    padding: 0;
}

.hero p {
    font-size: 1.25rem;
    color: var(--text-secondary);
    max-width: 600px;
    margin: 0 auto;
}

.doc-sections {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
    gap: 1.5rem;
    margin-top: 2rem;
}

.doc-card {
    background: var(--bg-primary);
    border: 1px solid var(--border-color);
    border-radius: 0.75rem;
    padding: 1.5rem;
    transition: all 0.3s ease;
    box-shadow: var(--shadow-sm);
}

.doc-card:hover {
    transform: translateY(-4px);
    box-shadow: var(--shadow-lg);
    border-color: var(--primary-light);
}

.doc-card h3 {
    color: var(--primary-color);
    font-size: 1.25rem;
    margin-top: 0;
    margin-bottom: 0.75rem;
}

.doc-card p {
    color: var(--text-secondary);
    font-size: 0.9rem;
    margin-bottom: 1rem;
}

.doc-card .links {
    display: flex;
    gap: 1rem;
    flex-wrap: wrap;
}

.doc-card .links a {
    font-size: 0.875rem;
    padding: 0.5rem 1rem;
    background: var(--bg-secondary);
    border-radius: 0.375rem;
    transition: all 0.2s ease;
}

.doc-card .links a:hover {
    background: var(--primary-color);
    color: white;
    text-decoration: none;
}

.section-title {
    font-size: 1.75rem;
    color: var(--primary-dark);
    margin-top: 3rem;
    margin-bottom: 1.5rem;
    padding-bottom: 0.5rem;
    border-bottom: 2px solid var(--border-color);
}

    </style>
</head>
<body>
    <header class="main-header">
        <div class="header-content">
            <div class="header-logo">Securaa<span>Docs</span></div>
            <div class="header-meta">
                <span>Generated: December 04, 2025</span>
            </div>
        </div>
    </header>

    <nav class="documentation-nav">
        <div class="nav-links">
            <a href="index.html">Home</a>
            <a href="securaa-platform-high-level-design.html">Platform</a>
            <a href="securaa-playbook-high-level-design.html">Playbook</a>
            <a href="securaa-siem-high-level-design.html">SIEM</a>
            <a href="securaa-user-high-level-design.html">User Service</a>
            <a href="securaa-custom-services-high-level-design.html">Custom Services</a>
            <a href="sia-service-high-level-design.html">SIA Service</a>
            <a href="securaa-ris-high-level-design.html">RIS</a>
        </div>
    </nav>

    <main class="main-content">
        <h1 id="low-level-design-lld-sia-service">Low-Level Design (LLD) - SIA Service</h1>
<h2 id="comprehensive-implementation-specification">Comprehensive Implementation Specification</h2>
<p><strong>Document Version</strong>: 1.0<br />
<strong>Last Updated</strong>: November 11, 2025<br />
<strong>Status</strong>: Active Development</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#1-component-architecture">Component Architecture</a></li>
<li><a href="#2-domain-models">Domain Models</a></li>
<li><a href="#3-database-schema">Database Schema</a></li>
<li><a href="#4-api-design">API Design</a></li>
<li><a href="#5-service-layer-implementation">Service Layer Implementation</a></li>
<li><a href="#6-repository-pattern">Repository Pattern</a></li>
<li><a href="#7-llm-integration">LLM Integration</a></li>
<li><a href="#8-message-processing">Message Processing</a></li>
<li><a href="#9-analysis-pipeline">Analysis Pipeline</a></li>
<li><a href="#10-false-positive-reduction">False Positive Reduction</a></li>
<li><a href="#11-implementation-diagrams">Implementation Diagrams</a></li>
<li><a href="#12-design-patterns">Design Patterns</a></li>
<li><a href="#13-error-handling">Error Handling</a></li>
<li><a href="#14-performance-optimizations">Performance Optimizations</a></li>
<li><a href="#15-testing-strategy">Testing Strategy</a></li>
<li><a href="#16-code-examples">Code Examples</a></li>
</ol>
<hr />
<h2 id="1-component-architecture">1. Component Architecture</h2>
<h3 id="11-fastapi-application-structure">1.1 FastAPI Application Structure</h3>
<pre class="highlight"><code class="language-python">main.py
├── Lifespan Management
│   └── init_tenant_sessions()
├── Router Registration
│   ├── users.router       (/user)
│   ├── chat.router        (/chat)
│   ├── case.router        (/case)
│   ├── index.router       (/indexing)
│   ├── tenant.router      (/tenant)
│   ├── application.router (/application)
│   └── code.router        (/code)
└── Middleware
    └── Traceloop (OpenTelemetry)
</code></pre>

<h3 id="12-module-organization">1.2 Module Organization</h3>
<pre class="highlight"><code>siaservice/
├── routers/              # API route handlers
├── apis/services/        # Business logic services
├── domain/soar/          # Domain models (Case, PlaybookTask, etc.)
├── repositories/         # Data access layer
├── db/
│   ├── pgv/             # PostgreSQL with pgvector
│   ├── embed/           # Embedding generation
│   └── vectordb/        # Vector database operations
├── llm/                 # LLM integration layer
├── analysis/            # Analysis engines and agents
├── parsers/             # Output parsers
├── noise_reduction/     # FPR pipeline
└── common.py            # Shared utilities
</code></pre>

<hr />
<h2 id="2-domain-models">2. Domain Models</h2>
<h3 id="21-case-model">2.1 Case Model</h3>
<pre class="highlight"><code class="language-python">class Case(BaseModel):
    &quot;&quot;&quot;
    Represents a security incident/case
    &quot;&quot;&quot;
    deployment_id: str      # Deployment identifier
    tenant_id: str          # Tenant identifier
    case_id: str           # Unique case identifier
    case_detail: str       # Detailed case description
    iocs: str              # Indicators of Compromise
    findings: Optional[List[Finding]] = None

    def to_dict(self) -&gt; dict
    @model_validator(mode='before')
    def check_non_empty_fields(cls, values)
</code></pre>

<p><strong>Validation Rules</strong>:
- All fields except <code>findings</code> are required
- No empty strings allowed for required fields
- Automatic validation via Pydantic</p>
<h3 id="22-playbooktask-model">2.2 PlaybookTask Model</h3>
<pre class="highlight"><code class="language-python">class PlaybookTask(BaseModel):
    &quot;&quot;&quot;
    Represents an executed task within a playbook
    &quot;&quot;&quot;
    deployment_id: str
    tenant_id: str
    case_id: str
    playbook_run_id: str
    task_details: dict      # Full task execution details

    def to_dict(self) -&gt; dict
    @model_validator(mode='before')
    def check_non_empty_fields(cls, values)
</code></pre>

<p><strong>Usage</strong>:
- Captures historical task executions
- Used for similarity search and context retrieval
- Indexed in vector database</p>
<h3 id="23-finding-model">2.3 Finding Model</h3>
<pre class="highlight"><code class="language-python">class Finding(BaseModel):
    &quot;&quot;&quot;
    Individual finding from case analysis
    &quot;&quot;&quot;
    source: str        # Source system or tool
    finding: str       # Description of the finding
</code></pre>

<hr />
<h2 id="3-database-schema">3. Database Schema</h2>
<h3 id="31-core-tables">3.1 Core Tables</h3>
<h4 id="cases">cases</h4>
<pre class="highlight"><code class="language-sql">CREATE TABLE cases (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    case_id VARCHAR(256) NOT NULL,
    deployment_id VARCHAR(256) NOT NULL,
    tenant_id VARCHAR(256) NOT NULL,
    iocs VARCHAR(1000),
    status VARCHAR(256),
    analyst_id INTEGER,
    text JSON NOT NULL,
    timeline_analysis JSON,
    playbook_recommendation JSON,
    analyst_recommendation JSON,
    investigation_report JSON,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_cases_case_id ON cases(case_id);
CREATE INDEX idx_cases_deployment_id ON cases(deployment_id);
CREATE INDEX idx_cases_tenant_id ON cases(tenant_id);
CREATE INDEX idx_cases_iocs ON cases(iocs);
CREATE INDEX idx_cases_status ON cases(status);
</code></pre>

<h4 id="case_summary">case_summary</h4>
<pre class="highlight"><code class="language-sql">CREATE TABLE case_summary (
    id SERIAL PRIMARY KEY,
    case_id UUID REFERENCES cases(id) ON DELETE CASCADE,
    summary JSON NOT NULL,
    version INTEGER DEFAULT 1,
    conclusion VARCHAR(256) DEFAULT 'unknown',
    risk_score FLOAT DEFAULT 0.0,
    embedding VECTOR(1536),  -- pgvector type
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_case_summary_conclusion ON case_summary(conclusion);
CREATE INDEX idx_case_summary_risk_score ON case_summary(risk_score);
CREATE INDEX embedding_case_summary_idx ON case_summary 
    USING hnsw (embedding vector_l2_ops) 
    WITH (m = 16, ef_construction = 64);
</code></pre>

<h4 id="playbook_tasks">playbook_tasks</h4>
<pre class="highlight"><code class="language-sql">CREATE TABLE playbook_tasks (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    case_id VARCHAR(256) NOT NULL,
    deployment_id VARCHAR(256) NOT NULL,
    tenant_id VARCHAR(256) NOT NULL,
    text JSON NOT NULL,
    embedding VECTOR(1536),
    text_summary VARCHAR(4096) NOT NULL,
    risk_score FLOAT DEFAULT 0.0,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_playbook_tasks_case_id ON playbook_tasks(case_id);
CREATE INDEX idx_playbook_tasks_risk_score ON playbook_tasks(risk_score);
CREATE INDEX embedding_hnsw_index ON playbook_tasks 
    USING hnsw (embedding vector_l2_ops) 
    WITH (m = 16, ef_construction = 64);
</code></pre>

<h4 id="task_classification">task_classification</h4>
<pre class="highlight"><code class="language-sql">CREATE TABLE task_classification (
    id SERIAL PRIMARY KEY,
    playbook_task_id UUID REFERENCES playbook_tasks(id) ON DELETE CASCADE,
    version INTEGER DEFAULT 1,
    lookup BOOLEAN NOT NULL,
    action BOOLEAN NOT NULL,
    decision BOOLEAN NOT NULL,
    playbook_name VARCHAR(256) NOT NULL,
    task_name VARCHAR(256) NOT NULL,
    case_id VARCHAR(256) NOT NULL,
    task_id VARCHAR(256) NOT NULL,
    playbook_id VARCHAR(256) NOT NULL,
    summary VARCHAR(1024),
    target JSON,
    source JSON,
    iocs JSON,
    result VARCHAR(1024),
    malicious BOOLEAN,
    malicious_reason VARCHAR(1024),
    timestamp BIGINT NOT NULL,
    embedding VECTOR(1536),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);
</code></pre>

<h3 id="32-chat-thread-tables">3.2 Chat &amp; Thread Tables</h3>
<h4 id="chathistory">chathistory</h4>
<pre class="highlight"><code class="language-sql">CREATE TABLE chathistory (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user VARCHAR NOT NULL,
    user_id INTEGER,
    created_ts TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_ts TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    message TEXT NOT NULL,
    deployment_id VARCHAR NOT NULL,
    tenant_id VARCHAR NOT NULL,
    thread_id UUID REFERENCES threads(thread_id) ON DELETE CASCADE,
    topic VARCHAR,
    entity VARCHAR NOT NULL,
    message_type VARCHAR NOT NULL
);

CREATE INDEX idx_chathistory_deployment_id ON chathistory(deployment_id);
CREATE INDEX idx_chathistory_tenant_id ON chathistory(tenant_id);
</code></pre>

<h4 id="threads">threads</h4>
<pre class="highlight"><code class="language-sql">CREATE TABLE threads (
    thread_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR(1024),
    deployment_id VARCHAR NOT NULL,
    tenant_id VARCHAR NOT NULL,
    case_id VARCHAR(256),
    user_id INTEGER,
    entity VARCHAR(256),
    created_ts TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_ts TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);
</code></pre>

<h3 id="33-definition-tables">3.3 Definition Tables</h3>
<h4 id="playbook_definitions">playbook_definitions</h4>
<pre class="highlight"><code class="language-sql">CREATE TABLE playbook_definitions (
    id SERIAL PRIMARY KEY,
    deployment_id VARCHAR(256) NOT NULL,
    tenant_id VARCHAR(256) NOT NULL,
    playbook_id INTEGER NOT NULL,
    description TEXT NOT NULL,
    name VARCHAR(255) NOT NULL,
    categoryname VARCHAR(255),
    summary JSON NOT NULL,
    embedding VECTOR(1536),
    eliminate_threat VARCHAR(256) DEFAULT 'No',
    restore_systems VARCHAR(256) DEFAULT 'No',
    enhance_measures VARCHAR(256) DEFAULT 'No',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_playbook_deployment_id ON playbook_definitions(deployment_id);
CREATE INDEX idx_playbook_eliminate_threat ON playbook_definitions(eliminate_threat);
</code></pre>

<h4 id="task_definitions">task_definitions</h4>
<pre class="highlight"><code class="language-sql">CREATE TABLE task_definitions (
    id SERIAL PRIMARY KEY,
    deployment_id VARCHAR(256) NOT NULL,
    tenant_id VARCHAR(256) NOT NULL,
    task_id INTEGER NOT NULL,
    task_type VARCHAR(255) NOT NULL,
    integration_id INTEGER,
    name VARCHAR(255),
    summary JSON,
    role VARCHAR(255),
    embedding VECTOR(1536),
    task_tag VARCHAR(255) NOT NULL,
    created_ts TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_ts TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_task_definitions_task_type ON task_definitions(task_type);
CREATE INDEX idx_task_definitions_task_tag ON task_definitions(task_tag);
</code></pre>

<h3 id="34-noise-reduction-tables">3.4 Noise Reduction Tables</h3>
<h4 id="case_suspicious_analysis">case_suspicious_analysis</h4>
<pre class="highlight"><code class="language-sql">CREATE TABLE case_suspicious_analysis (
    id SERIAL PRIMARY KEY,
    case_id UUID REFERENCES cases(id) ON DELETE CASCADE UNIQUE,
    deployment_id VARCHAR(256) NOT NULL,
    tenant_id VARCHAR(256) NOT NULL,
    is_novel BOOLEAN NOT NULL,
    cluster_id VARCHAR(256) NOT NULL,
    suspicious_score FLOAT,
    suspicious_reason VARCHAR(512),
    enriched_case JSON,
    is_fp BOOLEAN,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX ix_case_suspicious_analysis_deployment_tenant_case 
    ON case_suspicious_analysis(deployment_id, tenant_id, case_id);
</code></pre>

<h4 id="event_streams-events">event_streams &amp; events</h4>
<pre class="highlight"><code class="language-sql">CREATE TABLE event_streams (
    stream_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR,
    window_length INTEGER NOT NULL,
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE events (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    stream_id UUID REFERENCES event_streams(stream_id),
    event_id VARCHAR NOT NULL,
    content JSON NOT NULL,
    tags JSON,
    embedding VECTOR(384),  -- MiniLM-L6-v2
    created_at TIMESTAMP DEFAULT NOW()
);
</code></pre>

<hr />
<h2 id="4-api-design">4. API Design</h2>
<h3 id="41-case-analysis-endpoints">4.1 Case Analysis Endpoints</h3>
<h4 id="post-caseanalysis">POST /case/analysis</h4>
<p><strong>Request</strong>:</p>
<pre class="highlight"><code class="language-json">{
  &quot;case&quot;: {
    &quot;deployment_id&quot;: &quot;prod-001&quot;,
    &quot;tenant_id&quot;: &quot;customer-123&quot;,
    &quot;case_id&quot;: &quot;CASE-2024-001&quot;,
    &quot;case_detail&quot;: &quot;Suspicious IP 211.226.165.33 detected...&quot;,
    &quot;iocs&quot;: &quot;211.226.165.33, malicious-domain.com&quot;,
    &quot;findings&quot;: [
      {
        &quot;source&quot;: &quot;VirusTotal&quot;,
        &quot;finding&quot;: &quot;High community score of 96/100&quot;
      }
    ]
  }
}
</code></pre>

<p><strong>Response</strong>:</p>
<pre class="highlight"><code class="language-json">{
  &quot;task_id&quot;: &quot;e09f6929-f562-4428-a155-0f3ccf74a1b1&quot;
}
</code></pre>

<h4 id="get-caseanalysistask_id">GET /case/analysis/{task_id}</h4>
<p><strong>Response</strong>:</p>
<pre class="highlight"><code class="language-json">{
  &quot;task_id&quot;: &quot;e09f6929-f562-4428-a155-0f3ccf74a1b1&quot;,
  &quot;task_status&quot;: &quot;SUCCESS&quot;,
  &quot;task_result&quot;: {
    &quot;result&quot;: {
      &quot;executive_summary&quot;: &quot;...&quot;,
      &quot;conclusion&quot;: {
        &quot;status&quot;: &quot;malicious&quot;,
        &quot;reason&quot;: &quot;...&quot;,
        &quot;score&quot;: 10
      },
      &quot;findings&quot;: [...],
      &quot;recommendations&quot;: [...]
    }
  }
}
</code></pre>

<h3 id="42-chat-endpoints">4.2 Chat Endpoints</h3>
<h4 id="post-chatcompletions">POST /chat/completions</h4>
<p><strong>Request</strong>:</p>
<pre class="highlight"><code class="language-json">{
  &quot;model&quot;: &quot;gpt-4&quot;,
  &quot;messages&quot;: [
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a security analyst assistant&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Analyze this case...&quot;}
  ],
  &quot;stream&quot;: true
}
</code></pre>

<p><strong>Response</strong> (Streaming):</p>
<pre class="highlight"><code>data: {&quot;chunk&quot;: &quot;Based on the analysis...&quot;}
data: {&quot;chunk&quot;: &quot; the IP address...&quot;}
data: {&quot;chunk&quot;: &quot; is malicious.&quot;}
</code></pre>

<h4 id="post-chatthread">POST /chat/thread</h4>
<p><strong>Request</strong>:</p>
<pre class="highlight"><code class="language-json">{
  &quot;deployment_id&quot;: &quot;prod-001&quot;,
  &quot;tenant_id&quot;: &quot;customer-123&quot;,
  &quot;user_id&quot;: 1,
  &quot;case_id&quot;: &quot;CASE-2024-001&quot;,
  &quot;entity&quot;: &quot;case&quot;,
  &quot;name&quot;: &quot;Investigation Thread&quot;
}
</code></pre>

<h3 id="43-indexing-endpoints">4.3 Indexing Endpoints</h3>
<h4 id="post-indexingcaseplaybooktask">POST /indexing/case/playbook/task</h4>
<p><strong>Request</strong>:</p>
<pre class="highlight"><code class="language-json">{
  &quot;tasks&quot;: [
    {
      &quot;deployment_id&quot;: &quot;prod-001&quot;,
      &quot;tenant_id&quot;: &quot;customer-123&quot;,
      &quot;case_id&quot;: &quot;CASE-2024-001&quot;,
      &quot;playbook_run_id&quot;: &quot;playbook-run-123&quot;,
      &quot;task_details&quot;: {
        &quot;task_id&quot;: &quot;task-456&quot;,
        &quot;name&quot;: &quot;Check IP Reputation&quot;,
        &quot;result&quot;: &quot;Malicious&quot;,
        ...
      }
    }
  ]
}
</code></pre>

<hr />
<h2 id="5-service-layer">5. Service Layer</h2>
<h3 id="51-caseanalysisservice">5.1 CaseAnalysisService</h3>
<pre class="highlight"><code class="language-python">class OpenAICaseAnalysisService:
    &quot;&quot;&quot;
    Main service for case analysis using LLM
    &quot;&quot;&quot;

    def __init__(self, playbook_task_repo: PlaybookTaskRepo, 
                 model_name: str = &quot;llama3.1:8b&quot;):
        self.model = LLMWrapper()
        self.parser = SelfRepairJsonOutputParser()
        self.templates_factory = AnalysisTemplatesFactory()
        self.case2template = Case2UserTemplateConverter()
        self._case_analysis = CaseAnalysis(...)

    @retry((NoneResultException), tries=3, delay=2, backoff=2)
    def run(self, case: Case) -&gt; Tuple[dict, bool]:
        &quot;&quot;&quot;
        Execute case analysis with retry logic

        Returns:
            Tuple[dict, bool]: (result, is_cached)
        &quot;&quot;&quot;
        result, existing = self._case_analysis.run(case)
        if result is None:
            raise NoneResultException()
        return result, existing
</code></pre>

<h3 id="52-caseanalysis-engine">5.2 CaseAnalysis Engine</h3>
<pre class="highlight"><code class="language-python">class CaseAnalysis:
    &quot;&quot;&quot;
    Core analysis logic with RAG (Retrieval-Augmented Generation)
    &quot;&quot;&quot;

    def run(self, case: Case) -&gt; Tuple[dict, bool]:
        # 1. Check cache (Redis)
        hash_value = generate_hash_value(case)
        cached_result = get_redis_value(hash_value)
        if cached_result:
            return cached_result, True

        # 2. Retrieve relevant context (Vector search)
        question = &quot;Find all threat intel lookup tasks...&quot;
        playbook_tasks = self.playbook_task_repo.search_by_case(
            question, case.deployment_id, case.tenant_id, 
            case.case_id, limit=50
        )

        # 3. Build prompt
        contexts = [task.text_summary for task in playbook_tasks]
        system_template = self.templates_factory.get_by_case(case)
        user_template = self.case2template.convert(case, contexts)

        # 4. Generate with LLM
        prompt = ChatPromptTemplate.from_messages([
            (&quot;system&quot;, system_template),
            (&quot;user&quot;, user_template)
        ])
        chain = prompt | self.model | self.parser
        result = chain.invoke({})

        # 5. Store in cache
        store_redis_value(hash_value, result)

        return result, False
</code></pre>

<h3 id="53-chatbotagent">5.3 ChatBotAgent</h3>
<pre class="highlight"><code class="language-python">class ChatBotAgent:
    &quot;&quot;&quot;
    Handles interactive chat with RAG context
    &quot;&quot;&quot;

    async def stream(self, db: Session, deployment_id: str, 
                    tenant_id: str, case_id: str, user_id: int,
                    user_name: str, thread_id: str, 
                    prompt: str) -&gt; AsyncGenerator[str, None]:
        &quot;&quot;&quot;
        Stream LLM response with context retrieval
        &quot;&quot;&quot;
        # 1. Retrieve chat history
        history = ChatHistory(...)
        messages = history.get_latest_chat_history(thread_id, 5, user_id)

        # 2. Search relevant context
        playbook_repo = PlaybookTaskRepo(db, embedding_client)
        contexts = playbook_repo.search_by_case(
            prompt, deployment_id, tenant_id, case_id, limit=10
        )

        # 3. Build RAG prompt
        context_str = &quot;\n&quot;.join([c.text_summary for c in contexts])
        full_prompt = f&quot;Context: {context_str}\n\nUser: {prompt}&quot;

        # 4. Stream response
        llm = LLMWrapper(streaming=True)
        async for chunk in llm.astream(full_prompt):
            yield chunk.content

        # 5. Store in history
        history.store_chat_history({
            &quot;thread_id&quot;: thread_id,
            &quot;user&quot;: user_name,
            &quot;message&quot;: prompt,
            &quot;response&quot;: accumulated_response
        })
</code></pre>

<hr />
<h2 id="6-repository-pattern">6. Repository Pattern</h2>
<h3 id="61-playbooktaskrepo">6.1 PlaybookTaskRepo</h3>
<pre class="highlight"><code class="language-python">class PlaybookTaskRepo:
    &quot;&quot;&quot;
    Repository for playbook task CRUD and vector search
    &quot;&quot;&quot;

    def __init__(self, session: Session, embedding: AIEmbedding):
        self.session = session
        self.embedding = embedding

    def add_task(self, task: PlaybookTask) -&gt; PlaybookTaskModel:
        &quot;&quot;&quot;Create new task with embedding&quot;&quot;&quot;
        text_summary = self._generate_summary(task.task_details)
        embedding_vector = self.embedding.embed_text(text_summary)

        db_task = PlaybookTaskModel(
            case_id=task.case_id,
            deployment_id=task.deployment_id,
            tenant_id=task.tenant_id,
            text=task.task_details,
            text_summary=text_summary,
            embedding=embedding_vector
        )
        self.session.add(db_task)
        self.session.commit()
        return db_task

    def search_by_case(self, query: str, deployment_id: str,
                      tenant_id: str, case_id: str, 
                      limit: int = 10) -&gt; List[PlaybookTaskModel]:
        &quot;&quot;&quot;Vector similarity search&quot;&quot;&quot;
        query_embedding = self.embedding.embed_text(query)

        results = self.session.query(PlaybookTaskModel).filter(
            PlaybookTaskModel.deployment_id == deployment_id,
            PlaybookTaskModel.tenant_id == tenant_id,
            PlaybookTaskModel.case_id == case_id
        ).order_by(
            PlaybookTaskModel.embedding.l2_distance(query_embedding)
        ).limit(limit).all()

        return results
</code></pre>

<h3 id="62-caserepo">6.2 CaseRepo</h3>
<pre class="highlight"><code class="language-python">class CaseRepo:
    &quot;&quot;&quot;
    Repository for case management
    &quot;&quot;&quot;

    def create_case(self, case: Case) -&gt; CaseModel:
        &quot;&quot;&quot;Create new case record&quot;&quot;&quot;
        db_case = CaseModel(
            case_id=case.case_id,
            deployment_id=case.deployment_id,
            tenant_id=case.tenant_id,
            iocs=case.iocs,
            text={&quot;case_detail&quot;: case.case_detail}
        )
        self.session.add(db_case)
        self.session.commit()
        return db_case

    def add_summary(self, case_uuid: UUID, summary: dict,
                   conclusion: str, risk_score: float) -&gt; CaseSummary:
        &quot;&quot;&quot;Add AI-generated summary&quot;&quot;&quot;
        embedding = self.embedding.embed_text(
            json.dumps(summary)
        )

        db_summary = CaseSummary(
            case_id=case_uuid,
            summary=summary,
            conclusion=conclusion,
            risk_score=risk_score,
            embedding=embedding,
            version=1
        )
        self.session.add(db_summary)
        self.session.commit()
        return db_summary
</code></pre>

<h3 id="63-chathistory-repo">6.3 ChatHistory Repo</h3>
<pre class="highlight"><code class="language-python">class ChatHistory:
    &quot;&quot;&quot;
    Repository for chat thread and message management
    &quot;&quot;&quot;

    def add_thread(self, data: dict) -&gt; dict:
        &quot;&quot;&quot;Create new chat thread&quot;&quot;&quot;
        thread = Threads(
            deployment_id=data[&quot;deployment_id&quot;],
            tenant_id=data[&quot;tenant_id&quot;],
            case_id=data.get(&quot;case_id&quot;, &quot;&quot;),
            user_id=data[&quot;user_id&quot;],
            entity=data.get(&quot;entity&quot;, &quot;case&quot;),
            name=data.get(&quot;name&quot;, f&quot;Thread {datetime.now()}&quot;)
        )
        self.session.add(thread)
        self.session.commit()
        return {&quot;thread_id&quot;: str(thread.thread_id)}

    def store_chat_history(self, data: dict) -&gt; dict:
        &quot;&quot;&quot;Store chat message&quot;&quot;&quot;
        chat = ChatHistoryModel(
            thread_id=UUID(data[&quot;thread_id&quot;]),
            user=data[&quot;user&quot;],
            user_id=data.get(&quot;user_id&quot;),
            message=data[&quot;message&quot;],
            deployment_id=data[&quot;deployment_id&quot;],
            tenant_id=data[&quot;tenant_id&quot;],
            entity=data.get(&quot;entity&quot;, &quot;case&quot;),
            message_type=data.get(&quot;message_type&quot;, &quot;text&quot;)
        )
        self.session.add(chat)
        self.session.commit()
        return {&quot;id&quot;: str(chat.id)}

    def get_chat_history(self, thread_id: str, skip: int, 
                        limit: int, user_id: int) -&gt; List[dict]:
        &quot;&quot;&quot;Retrieve paginated chat history&quot;&quot;&quot;
        results = self.session.query(ChatHistoryModel).filter(
            ChatHistoryModel.thread_id == UUID(thread_id)
        ).order_by(
            ChatHistoryModel.created_ts.desc()
        ).offset(skip).limit(limit).all()

        return [self._to_dict(r) for r in results]
</code></pre>

<hr />
<h2 id="7-llm-integration">7. LLM Integration</h2>
<h3 id="71-llmwrapper">7.1 LLMWrapper</h3>
<pre class="highlight"><code class="language-python">class LLMWrapper(Runnable):
    &quot;&quot;&quot;
    Unified LLM interface supporting multiple backends
    &quot;&quot;&quot;

    def __init__(self, model: str = os.getenv('MODEL_NAME'),
                 backend: str = os.getenv('LLM_BACKEND'),
                 temperature: float = 0.0,
                 streaming: bool = False):
        self.backend = backend
        self.model = model
        self.temperature = temperature
        self.streaming = streaming

        if backend == &quot;OPENAI&quot;:
            self.llm = ChatOpenAI(
                model=model,
                temperature=temperature,
                streaming=streaming
            )
        else:  # On-premise (Ollama)
            self.llm = ChatOllama(
                model=model,
                base_url=os.getenv(&quot;ONPREM_MODEL_BASE_URL&quot;),
                temperature=temperature,
                streaming=streaming
            )

    def invoke(self, prompt: Union[str, List[BaseMessage]], 
              *args, **kwargs) -&gt; Any:
        &quot;&quot;&quot;Synchronous invocation&quot;&quot;&quot;
        messages = self._prepare_messages(prompt)
        response = self.llm.invoke(messages, *args, **kwargs)
        if self.structured_parser:
            return self.structured_parser.parse(response.content)
        return response

    async def astream(self, prompt: Union[str, List[BaseMessage]], 
                     *args, **kwargs) -&gt; AsyncGenerator[Any, None]:
        &quot;&quot;&quot;Asynchronous streaming&quot;&quot;&quot;
        messages = self._prepare_messages(prompt)
        async for chunk in self.llm.astream(messages, *args, **kwargs):
            if self.structured_parser:
                yield self.structured_parser.parse(chunk)
            else:
                yield chunk

    def with_structured_output(self, output_class: Type[Any]):
        &quot;&quot;&quot;Enable structured output parsing&quot;&quot;&quot;
        self.structured_parser = PydanticOutputParser(
            pydantic_object=output_class
        )
        return self
</code></pre>

<h3 id="72-aiembedding">7.2 AIEmbedding</h3>
<pre class="highlight"><code class="language-python">class AIEmbedding:
    &quot;&quot;&quot;
    Embedding generation with multiple backends
    &quot;&quot;&quot;

    def __init__(self):
        self.backend = os.getenv(&quot;EMBEDDING_BACKEND&quot;, &quot;OPENAI&quot;)
        self.model = os.getenv(&quot;EMBEDDING_MODEL_NAME&quot;)

        if self.backend == &quot;OPENAI&quot;:
            self.client = OpenAI()
        else:
            self.base_url = os.getenv(&quot;ONPREM_EMBEDDING_URL&quot;)

    def embed_text(self, text: str) -&gt; List[float]:
        &quot;&quot;&quot;Generate embedding vector&quot;&quot;&quot;
        if self.backend == &quot;OPENAI&quot;:
            response = self.client.embeddings.create(
                model=self.model,
                input=text
            )
            return response.data[0].embedding
        else:
            # On-premise embedding
            response = requests.post(
                f&quot;{self.base_url}/embed&quot;,
                json={&quot;text&quot;: text}
            )
            return response.json()[&quot;embedding&quot;]

    def embed_batch(self, texts: List[str]) -&gt; List[List[float]]:
        &quot;&quot;&quot;Batch embedding generation&quot;&quot;&quot;
        return [self.embed_text(text) for text in texts]
</code></pre>

<hr />
<h2 id="8-message-processing">8. Message Processing</h2>
<h3 id="81-kafka-consumer-indexer">8.1 Kafka Consumer (Indexer)</h3>
<pre class="highlight"><code class="language-python">class KafkaConsumer_PgvIndexer:
    &quot;&quot;&quot;
    Consumes indexing messages and stores in PostgreSQL
    &quot;&quot;&quot;

    def __init__(self, uri: str, embedding: AIEmbedding,
                 upstream_kafka_producer: FpProcessingProducer):
        self.uri = uri
        self.embedding = embedding
        self.upstream_producer = upstream_kafka_producer

    def subscribe_and_index(self, topic: str):
        &quot;&quot;&quot;Main processing loop&quot;&quot;&quot;
        consumer = KafkaConsumer(
            topic,
            bootstrap_servers=[self.uri],
            value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            auto_offset_reset='earliest'
        )

        for message in consumer:
            tasks = message.value
            for task_dict in tasks:
                self._process_task(task_dict)

    def _process_task(self, task_dict: dict):
        &quot;&quot;&quot;Process individual task&quot;&quot;&quot;
        task = PlaybookTask(**task_dict)

        # Generate embedding
        text_summary = self._summarize_task(task.task_details)
        embedding = self.embedding.embed_text(text_summary)

        # Store in database
        db = get_session_for_tenant(task.tenant_id)
        try:
            repo = PlaybookTaskRepo(db, self.embedding)
            db_task = repo.add_task(task)

            # Forward to FPR processing
            self.upstream_producer.publish({
                &quot;task_id&quot;: str(db_task.id),
                &quot;case_id&quot;: task.case_id,
                &quot;tenant_id&quot;: task.tenant_id,
                &quot;deployment_id&quot;: task.deployment_id
            })
        finally:
            db.close()
</code></pre>

<h3 id="82-celery-worker">8.2 Celery Worker</h3>
<pre class="highlight"><code class="language-python">@celery.task(name=&quot;create_case_analysis_task&quot;)
def create_case_analysis_task(case_dict: dict):
    &quot;&quot;&quot;
    Background task for case analysis
    &quot;&quot;&quot;
    case = Case(**case_dict)
    logger.info(f&quot;Processing case {case.case_id}&quot;)

    # Get database session for tenant
    with db_session(case.tenant_id) as session:
        # Initialize services
        playbook_task_repo = PlaybookTaskRepo(
            session=session,
            embedding=embedding_client
        )
        case_service = OpenAICaseAnalysisService(
            playbook_task_repo=playbook_task_repo
        )

        # Run analysis
        result, existing = case_service.run(case)

        # Store result
        case_repo = CaseRepo(session)
        case_repo.add_summary(
            case.case_id,
            result,
            result[&quot;conclusion&quot;][&quot;status&quot;],
            result[&quot;conclusion&quot;][&quot;score&quot;]
        )

        # Trigger risk scoring (async)
        if not existing:
            set_risk.delay(
                case.deployment_id,
                case.tenant_id,
                case.case_id,
                case.case_detail,
                case.iocs,
                result
            )

        return {&quot;result&quot;: result}
</code></pre>

<hr />
<h2 id="9-analysis-pipeline">9. Analysis Pipeline</h2>
<h3 id="91-template-factory">9.1 Template Factory</h3>
<pre class="highlight"><code class="language-python">class AnalysisTemplatesFactory:
    &quot;&quot;&quot;
    Provides analysis prompt templates
    &quot;&quot;&quot;

    def get_by_case(self, case: Case) -&gt; str:
        &quot;&quot;&quot;
        Returns system prompt based on case type
        &quot;&quot;&quot;
        return &quot;&quot;&quot;
You are an expert security analyst. Your task is to analyze 
security cases and provide:

1. Executive Summary: Brief overview of the incident
2. Findings: Detailed findings from each source
3. Conclusion: Assessment (malicious/benign/suspicious) with score
4. Recommendations: Actionable remediation steps

Use the provided context from historical tasks to inform your analysis.
Output must be valid JSON following this structure:
{
  &quot;executive_summary&quot;: &quot;...&quot;,
  &quot;findings&quot;: [...],
  &quot;conclusion&quot;: {&quot;status&quot;: &quot;...&quot;, &quot;reason&quot;: &quot;...&quot;, &quot;score&quot;: 0-10},
  &quot;recommendations&quot;: [...]
}
&quot;&quot;&quot;
</code></pre>

<h3 id="92-output-parser">9.2 Output Parser</h3>
<pre class="highlight"><code class="language-python">class SelfRepairJsonOutputParser(JsonOutputParser):
    &quot;&quot;&quot;
    JSON parser with self-repair capability
    &quot;&quot;&quot;

    def parse(self, text: str) -&gt; dict:
        &quot;&quot;&quot;
        Parse LLM output with error recovery
        &quot;&quot;&quot;
        try:
            # Try standard JSON parsing
            return json.loads(text)
        except json.JSONDecodeError as e:
            logger.warning(f&quot;JSON parse error: {e}&quot;)

            # Try to repair JSON
            from json_repair import repair_json
            try:
                repaired = repair_json(text)
                return json.loads(repaired)
            except Exception as e2:
                logger.error(f&quot;JSON repair failed: {e2}&quot;)

                # Last resort: extract JSON from markdown
                import re
                json_match = re.search(
                    r'```json\s*(\{.*?\})\s*```',
                    text,
                    re.DOTALL
                )
                if json_match:
                    return json.loads(json_match.group(1))

                raise ValueError(&quot;Unable to parse JSON response&quot;)
</code></pre>

<h3 id="93-risk-scoring">9.3 Risk Scoring</h3>
<pre class="highlight"><code class="language-python">def analyze_risk_and_recommendation(session: Session,
                                   deployment_id: str,
                                   tenant_id: str,
                                   case_id: str,
                                   case_detail: str,
                                   iocs: str,
                                   summary: dict,
                                   is_novel: bool,
                                   cluster_id: str,
                                   case_uuid: str):
    &quot;&quot;&quot;
    Calculate risk score and generate recommendations
    &quot;&quot;&quot;
    # 1. Extract features
    conclusion = summary.get(&quot;conclusion&quot;, {})
    base_score = conclusion.get(&quot;score&quot;, 5)
    status = conclusion.get(&quot;status&quot;, &quot;unknown&quot;)

    # 2. Adjust score based on factors
    risk_score = base_score

    if status == &quot;malicious&quot;:
        risk_score = min(risk_score + 2, 10)
    elif status == &quot;suspicious&quot;:
        risk_score = min(risk_score + 1, 10)

    if is_novel:
        risk_score = min(risk_score + 1, 10)

    if len(iocs) &gt; 100:  # Multiple IOCs
        risk_score = min(risk_score + 0.5, 10)

    # 3. Generate recommendations
    recommendations = []
    if risk_score &gt;= 8:
        recommendations.append({
            &quot;priority&quot;: &quot;critical&quot;,
            &quot;action&quot;: &quot;Immediate investigation required&quot;
        })
    elif risk_score &gt;= 6:
        recommendations.append({
            &quot;priority&quot;: &quot;high&quot;,
            &quot;action&quot;: &quot;Review within 4 hours&quot;
        })

    # 4. Store in database
    case_repo = CaseRepo(session)
    case_repo.update_risk_score(case_uuid, risk_score)
    case_repo.add_recommendations(case_uuid, recommendations)
</code></pre>

<hr />
<h2 id="10-false-positive-reduction">10. False Positive Reduction</h2>
<h3 id="101-fpr-worker">10.1 FPR Worker</h3>
<pre class="highlight"><code class="language-python">class CaseFPReductionWorker:
    &quot;&quot;&quot;
    Main worker for false positive reduction
    &quot;&quot;&quot;

    def __init__(self, config: FPReductionConfig):
        self.config = config
        self.nds_client = NDSClient()
        self.embedding_client = AIEmbedding()
        self.batch_buffer = defaultdict(list)

    def start_fpr_case_processor(self):
        &quot;&quot;&quot;Process enriched cases&quot;&quot;&quot;
        consumer = KafkaConsumer(
            self.config.processing_topic,
            bootstrap_servers=[self.config.kafka_broker],
            value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            group_id=self.config.processing_group_id
        )

        for msg in consumer:
            case = msg.value
            tenant_id = case.get(&quot;tenant_id&quot;)

            # Buffer cases by tenant
            self.batch_buffer[tenant_id].append(case)

            # Process when batch is full
            if len(self.batch_buffer[tenant_id]) &gt;= self.config.batch_size:
                self.process_tenant_batch(
                    case[&quot;deployment_id&quot;],
                    tenant_id,
                    self.batch_buffer[tenant_id]
                )
                self.batch_buffer[tenant_id] = []

    def process_tenant_batch(self, deployment_id: str,
                            tenant_id: str,
                            cases: List[dict]):
        &quot;&quot;&quot;Process batch of cases&quot;&quot;&quot;
        db = get_session_for_tenant(tenant_id)
        try:
            for case in cases:
                # 1. Build features
                features = FeatureBuilderModuleV2.build(case)

                # 2. Check novelty
                novelty_result = self.nds_client.check_novelty(
                    deployment_id,
                    tenant_id,
                    case,
                    self.config.nds_window_length
                )

                # 3. Score suspiciousness
                score, reason = SuspiciousScoringModule.score(
                    case,
                    features,
                    novelty_result
                )

                # 4. Classify as FP or legit
                is_fp = self._classify_fp(score, novelty_result)

                # 5. Store results
                upsert_case_suspicious_analysis(
                    db,
                    case[&quot;case_id&quot;],
                    deployment_id,
                    tenant_id,
                    novelty_result[&quot;is_novel&quot;],
                    novelty_result[&quot;cluster_id&quot;],
                    score,
                    reason,
                    is_fp
                )

                # 6. Update case status
                if is_fp:
                    set_case_as_noise(
                        db,
                        deployment_id,
                        tenant_id,
                        case[&quot;case_id&quot;]
                    )
        finally:
            db.close()

    def _classify_fp(self, score: float, novelty: dict) -&gt; bool:
        &quot;&quot;&quot;Determine if case is false positive&quot;&quot;&quot;
        if not novelty[&quot;is_novel&quot;] and score &lt; 3.0:
            return True
        if score &lt; 1.0:
            return True
        return False
</code></pre>

<h3 id="102-feature-builder">10.2 Feature Builder</h3>
<pre class="highlight"><code class="language-python">class FeatureBuilderModuleV2:
    &quot;&quot;&quot;
    Extract ML features from case data
    &quot;&quot;&quot;

    @staticmethod
    def build(case: dict) -&gt; dict:
        &quot;&quot;&quot;
        Build feature vector
        &quot;&quot;&quot;
        features = {
            # IOC features
            &quot;num_iocs&quot;: len(case.get(&quot;iocs&quot;, &quot;&quot;).split(&quot;,&quot;)),
            &quot;has_ip&quot;: bool(re.search(r'\d+\.\d+\.\d+\.\d+', case.get(&quot;iocs&quot;, &quot;&quot;))),
            &quot;has_domain&quot;: bool(re.search(r'[a-z0-9-]+\.[a-z]{2,}', case.get(&quot;iocs&quot;, &quot;&quot;))),
            &quot;has_hash&quot;: bool(re.search(r'[a-f0-9]{32,64}', case.get(&quot;iocs&quot;, &quot;&quot;))),

            # Case features
            &quot;case_length&quot;: len(case.get(&quot;case_detail&quot;, &quot;&quot;)),
            &quot;num_findings&quot;: len(case.get(&quot;findings&quot;, [])),

            # Temporal features
            &quot;hour_of_day&quot;: datetime.now().hour,
            &quot;day_of_week&quot;: datetime.now().weekday(),

            # Text features
            &quot;contains_malicious&quot;: &quot;malicious&quot; in case.get(&quot;case_detail&quot;, &quot;&quot;).lower(),
            &quot;contains_suspicious&quot;: &quot;suspicious&quot; in case.get(&quot;case_detail&quot;, &quot;&quot;).lower(),
        }

        return features
</code></pre>

<h3 id="103-novelty-detection-client">10.3 Novelty Detection Client</h3>
<pre class="highlight"><code class="language-python">class NDSClient:
    &quot;&quot;&quot;
    Client for Novelty Detection Service
    &quot;&quot;&quot;

    def __init__(self):
        self.base_url = os.getenv(&quot;NDS_API_URL&quot;)

    def check_novelty(self, deployment_id: str, tenant_id: str,
                     case: dict, window_length: int) -&gt; dict:
        &quot;&quot;&quot;
        Check if case is novel
        &quot;&quot;&quot;
        # Generate embedding for case
        embedding = self._generate_embedding(case)

        # Call NDS API
        response = requests.post(
            f&quot;{self.base_url}/api/v1/novelty/check&quot;,
            json={
                &quot;stream_id&quot;: f&quot;{deployment_id}_{tenant_id}&quot;,
                &quot;event_id&quot;: case[&quot;case_id&quot;],
                &quot;content&quot;: case,
                &quot;embedding&quot;: embedding,
                &quot;window_length&quot;: window_length
            }
        )

        result = response.json()
        return {
            &quot;is_novel&quot;: result[&quot;is_novel&quot;],
            &quot;similarity_score&quot;: result[&quot;similarity_score&quot;],
            &quot;similar_cases&quot;: result.get(&quot;similar_event_ids&quot;, []),
            &quot;cluster_id&quot;: result.get(&quot;cluster_id&quot;, &quot;&quot;)
        }
</code></pre>

<hr />
<h2 id="11-implementation-diagrams">11. Implementation Diagrams</h2>
<h3 id="111-component-interaction-diagram">11.1 Component Interaction Diagram</h3>
<div class="mermaid">
graph TB
    subgraph FastAPIApp["FastAPI Application"]
        MAIN[main.py<br/>Application Entry]

        subgraph Routers
            R1[case.router]
            R2[chat.router]
            R3[index.router]
            R4[user.router]
            R5[tenant.router]
            R6[application.router]
            R7[code.router]
        end

        subgraph Services
            S1[CaseAnalysisService]
            S2[ChatBotAgent]
            S3[IndexingService]
            S4[CaseInformation]
            S5[UserInfo]
            S6[TenantManager]
        end

        subgraph Repositories
            REP1[CaseRepo]
            REP2[PlaybookTaskRepo]
            REP3[ChatHistory]
            REP4[UserRepo]
            REP5[TenantRepo]
            REP6[ApplicationRepo]
        end

        MAIN --> R1
        MAIN --> R2
        MAIN --> R3
        MAIN --> R4
        MAIN --> R5
        MAIN --> R6
        MAIN --> R7

        R1 --> S1
        R2 --> S2
        R3 --> S3
        R1 --> S4
        R4 --> S5
        R5 --> S6

        S1 --> REP1
        S1 --> REP2
        S2 --> REP2
        S2 --> REP3
        S4 --> REP1
        S5 --> REP4
        S6 --> REP5
    end

    subgraph DatabaseLayer["Database Layer"]
        DB[(PostgreSQL)]
        CACHE[(Redis)]
    end

    REP1 --> DB
    REP2 --> DB
    REP3 --> DB
    REP4 --> DB
    REP5 --> DB
    REP6 --> DB

    S1 --> CACHE
    S2 --> CACHE

    classDef entry fill:#ffcc99,stroke:#333,stroke-width:2px
    classDef database fill:#99ff99,stroke:#333,stroke-width:2px
    classDef cache fill:#ff9999,stroke:#333,stroke-width:2px

    class MAIN entry
    class DB database
    class CACHE cache
</div>

<h3 id="112-llm-integration-architecture">11.2 LLM Integration Architecture</h3>
<div class="mermaid">
graph TB
    subgraph AppLayer["Application Layer"]
        SRVICE[Analysis Services]
        CHAT[Chat Services]
    end

    subgraph WrapperLayer["LLM Wrapper Layer"]
        WRAPPER[LLMWrapper<br/>Unified Interface]

        subgraph Configuration
            CONFIG{LLM_BACKEND}
        end

        subgraph Implementations
            OPENAI[ChatOpenAI]
            OLLAMA[ChatOllama]
        end
    end

    subgraph Features
        STREAM[Streaming Support]
        STRUCT[Structured Output]
        PARSE[Output Parsing]
        RETRY[Retry Logic]
    end

    subgraph LLMProviders["External LLM Providers"]
        GPT[OpenAI<br/>GPT-4/3.5]
        GROQ[Groq<br/>Llama3]
        GEMINI[Google<br/>Gemini]
        ONPREM[On-Premise<br/>Ollama]
    end

    subgraph Security
        DTX[DTX Prompt Guard<br/>Injection Prevention]
    end

    SRVICE --> WRAPPER
    CHAT --> WRAPPER

    WRAPPER --> CONFIG
    CONFIG -->|OPENAI| OPENAI
    CONFIG -->|ONPREM| OLLAMA

    WRAPPER --> STREAM
    WRAPPER --> STRUCT
    WRAPPER --> PARSE
    WRAPPER --> RETRY

    OPENAI --> GPT
    OPENAI --> GEMINI
    OPENAI --> GROQ
    OLLAMA --> ONPREM

    SRVICE --> DTX
    CHAT --> DTX

    classDef wrapper fill:#99ccff,stroke:#333,stroke-width:2px
    classDef config fill:#ffcc99,stroke:#333,stroke-width:2px
    classDef security fill:#ff9999,stroke:#333,stroke-width:2px

    class WRAPPER wrapper
    class CONFIG config
    class DTX security
</div>

<h3 id="113-vector-search-implementation">11.3 Vector Search Implementation</h3>
<div class="mermaid">
flowchart LR
    subgraph "Input"
        TEXT[Text Input<br/>Task/Case/Query]
    end

    subgraph "Embedding Generation"
        CHECK{Embedding<br/>Backend}
        OPENAI_EMBED[OpenAI<br/>text-embedding-ada-002]
        ONPREM_EMBED[On-Premise<br/>Embedding API]
    end

    subgraph "Vector Storage"
        PG[(PostgreSQL<br/>pgvector)]
        CHROMA[(ChromaDB<br/>Optional)]
    end

    subgraph "Vector Search"
        QUERY[Query Vector]
        SIMILARITY[Calculate Similarity<br/>L2 Distance]
        HNSW[HNSW Index<br/>Approximate NN]
        FILTER[Apply Filters<br/>tenant_id, case_id]
    end

    subgraph "Output"
        RESULTS[Top-K Results]
        RERANK[Optional Re-ranking]
        FINAL[Final Context]
    end

    TEXT --> CHECK
    CHECK -->|OPENAI| OPENAI_EMBED
    CHECK -->|ONPREM| ONPREM_EMBED

    OPENAI_EMBED --> VECTOR[Embedding Vector<br/>1536 dimensions]
    ONPREM_EMBED --> VECTOR

    VECTOR --> PG
    VECTOR -.-> CHROMA

    QUERY --> SIMILARITY
    SIMILARITY --> HNSW
    HNSW --> PG
    HNSW -.-> CHROMA

    PG --> FILTER
    CHROMA -.-> FILTER

    FILTER --> RESULTS
    RESULTS --> RERANK
    RERANK --> FINAL

    style TEXT fill:#99ff99
    style VECTOR fill:#ffcc99
    style PG fill:#99ccff
    style FINAL fill:#ff99ff
</div>

<h3 id="114-celery-task-state-machine">11.4 Celery Task State Machine</h3>
<div class="mermaid">
stateDiagram-v2
    [*] --> Pending: Task Queued

    Pending --> Running: Worker Picks Up

    Running --> CheckCache: Start Processing

    CheckCache --> CacheHit: Found in Redis
    CheckCache --> CacheMiss: Not Found

    CacheHit --> Success: Return Cached

    CacheMiss --> VectorSearch: Retrieve Context
    VectorSearch --> LLMGeneration: Build Prompt
    LLMGeneration --> Parsing: Parse Response

    Parsing --> ParseSuccess: Valid JSON
    Parsing --> ParseFailed: Invalid JSON

    ParseFailed --> Retry: Attempt Repair
    Retry --> Parsing: Re-parse
    Retry --> Failure: Max Retries

    ParseSuccess --> StoreDB: Save Results
    StoreDB --> StoreCache: Cache for Future
    StoreCache --> TriggerRisk: Background Task
    TriggerRisk --> Success: Complete

    Success --> [*]
    Failure --> [*]
</div>

<h3 id="115-kafka-topic-flow">11.5 Kafka Topic Flow</h3>
<div class="mermaid">
flowchart LR
    subgraph Producers
        CLI[CLI Indexer]
        API[API Service]
        INDEXER[Kafka Indexer]
        FPR[FPR Processor]
    end

    subgraph Topics
        T1[(indexing-topic)]
        T2[(fpr-processing)]
        T3[(fpr-results)]
    end

    subgraph Consumers
        C1[Kafka Indexer]
        C2[FPR Processor]
        C3[Results Consumer]
    end

    subgraph ConsumerGroups["Consumer Groups"]
        G1[indexing_group]
        G2[fp_reduction_group]
        G3[results_group]
    end

    CLI --> T1
    API --> T1

    T1 --> C1
    C1 -.-> G1

    INDEXER --> T2

    T2 --> C2
    C2 -.-> G2

    FPR --> T3

    T3 --> C3
    C3 -.-> G3

    classDef topic fill:#ffcc99,stroke:#333,stroke-width:2px
    classDef consumer fill:#99ccff,stroke:#333,stroke-width:2px

    class T1,T2,T3 topic
    class C1,C2,C3 consumer
</div>

<h3 id="116-sequence-diagram-case-analysis-flow">11.6 Sequence Diagram: Case Analysis Flow</h3>
<div class="mermaid">
sequenceDiagram
    participant Client
    participant TykGW as Tyk Gateway
    participant FastAPI
    participant Celery
    participant Worker
    participant DB as PostgreSQL
    participant LLM
    participant Redis

    Client->>TykGW: POST /case/analysis
    TykGW->>FastAPI: Authenticate & Forward
    FastAPI->>Celery: Queue Task
    Celery-->>FastAPI: task_id
    FastAPI-->>Client: {task_id}

    Worker->>Redis: Check Cache
    alt Cache Hit
        Redis-->>Worker: Cached Result
    else Cache Miss
        Worker->>DB: Search Similar Tasks
        DB-->>Worker: Context Tasks
        Worker->>LLM: Generate Analysis
        LLM-->>Worker: Analysis Result
        Worker->>Redis: Store Cache
    end

    Worker->>DB: Store Summary
    Worker->>Celery: Trigger Risk Scoring

    Client->>FastAPI: GET /case/analysis/{task_id}
    FastAPI->>Celery: Get Result
    Celery-->>FastAPI: Result
    FastAPI-->>Client: Analysis Result
</div>

<h3 id="112-task-indexing-flow">11.2 Task Indexing Flow</h3>
<div class="mermaid">
sequenceDiagram
    participant CLI
    participant Kafka
    participant Indexer
    participant Embed as Embedding Service
    participant DB as PostgreSQL
    participant FPR as FPR Processor

    CLI->>Kafka: Publish Tasks
    Kafka-->>Indexer: Consume Message

    loop For Each Task
        Indexer->>Indexer: Extract Text
        Indexer->>Embed: Generate Embedding
        Embed-->>Indexer: Vector
        Indexer->>DB: Store Task + Vector
        Indexer->>Kafka: Publish to FPR Topic
    end

    Kafka-->>FPR: Consume Enriched Case
    FPR->>FPR: Build Features
    FPR->>FPR: Check Novelty
    FPR->>FPR: Score Suspiciousness
    FPR->>DB: Update Analysis
</div>

<h3 id="113-chat-flow-with-rag">11.3 Chat Flow with RAG</h3>
<div class="mermaid">
sequenceDiagram
    participant Client
    participant FastAPI
    participant ChatAgent
    participant DB as PostgreSQL
    participant VectorDB
    participant LLM

    Client->>FastAPI: POST /chat/completions (stream=true)
    FastAPI->>ChatAgent: Process Request

    ChatAgent->>DB: Get Thread History
    DB-->>ChatAgent: Recent Messages

    ChatAgent->>VectorDB: Semantic Search
    VectorDB-->>ChatAgent: Relevant Context

    ChatAgent->>ChatAgent: Build RAG Prompt
    ChatAgent->>LLM: Stream Request

    loop Streaming
        LLM-->>ChatAgent: Chunk
        ChatAgent-->>FastAPI: Chunk
        FastAPI-->>Client: Stream Chunk
    end

    ChatAgent->>DB: Store Message
</div>

<h3 id="114-repository-pattern-class-diagram">11.4 Repository Pattern Class Diagram</h3>
<div class="mermaid">
classDiagram
    class BaseRepository~T~ {
        <<abstract>>
        #db: Session
        #tenant_id: str
        +create(entity: T) T
        +get(id: str) Optional~T~
        +update(entity: T) T
        +delete(id: str) bool
        +list(filters: dict) List~T~
    }

    class CaseRepository {
        +get_by_case_id(case_id: str) Case
        +get_by_status(status: str) List~Case~
        +update_summary(case_id: str, summary: str)
        +update_risk(case_id: str, risk: str)
        +set_as_noise(case_id: str)
    }

    class PlaybookTaskRepository {
        +search_by_case(query: str, case_id: str, limit: int) List~Task~
        +search_by_deployment(query: str, deployment_id: str) List~Task~
        +get_by_embedding(vector: List~float~) List~Task~
        +bulk_insert(tasks: List~Task~)
    }

    class ChatHistoryRepository {
        +create_thread(case_id: str, user_id: int) Thread
        +add_message(thread_id: str, role: str, content: str) Message
        +get_history(thread_id: str, limit: int) List~Message~
        +update_message(msg_id: str, content: str)
    }

    class TaskClassificationRepository {
        +classify_task(task_id: str, category: str)
        +get_classification(task_id: str) Classification
        +bulk_classify(classifications: List)
    }

    class CaseSummaryRepository {
        +store_summary(case_id: str, summary: dict)
        +get_summary(case_id: str) dict
        +update_analysis(case_id: str, analysis: dict)
    }

    BaseRepository <|-- CaseRepository
    BaseRepository <|-- PlaybookTaskRepository
    BaseRepository <|-- ChatHistoryRepository
    BaseRepository <|-- TaskClassificationRepository
    BaseRepository <|-- CaseSummaryRepository

    class EmbeddingClient {
        <<interface>>
        +create_embedding(text: str) List~float~
        +batch_create(texts: List~str~) List~List~float~~
    }

    class OpenAIEmbedding
    class OnPremiseEmbedding

    EmbeddingClient <|.. OpenAIEmbedding
    EmbeddingClient <|.. OnPremiseEmbedding

    PlaybookTaskRepository --> EmbeddingClient : uses
</div>

<h3 id="115-tenant-onboarding-workflow">11.5 Tenant Onboarding Workflow</h3>
<div class="mermaid">
sequenceDiagram
    participant Admin
    participant API as API Gateway
    participant TenantSvc as Tenant Service
    participant DB as PostgreSQL
    participant Kafka
    participant IndexSvc as Indexing Service

    Admin->>API: POST /tenants<br/>{name, config}
    API->>TenantSvc: create_tenant(tenant_info)

    TenantSvc->>DB: Create siadb_<tenant_id>
    DB-->>TenantSvc: Database created

    TenantSvc->>DB: Run Alembic migrations
    DB-->>TenantSvc: Schema initialized

    TenantSvc->>DB: INSERT INTO tenants
    DB-->>TenantSvc: Tenant record created

    TenantSvc->>Kafka: Publish tenant_created event

    TenantSvc-->>API: {tenant_id, status: "active"}
    API-->>Admin: Tenant created successfully

    Kafka->>IndexSvc: tenant_created event
    IndexSvc->>IndexSvc: Create embedding collections
    IndexSvc->>DB: Initialize vector indexes

    Note over IndexSvc: Background initialization<br/>completes asynchronously
</div>

<h3 id="116-error-handling-flow">11.6 Error Handling Flow</h3>
<div class="mermaid">
flowchart TD
    START[API Request] --> VALIDATE{Valid<br/>Request?}

    VALIDATE -->|No| ERROR_400[Return 400<br/>Bad Request]
    VALIDATE -->|Yes| AUTH{Authenticated?}

    AUTH -->|No| ERROR_401[Return 401<br/>Unauthorized]
    AUTH -->|Yes| AUTHZ{Authorized?}

    AUTHZ -->|No| ERROR_403[Return 403<br/>Forbidden]
    AUTHZ -->|Yes| PROCESS[Process Request]

    PROCESS --> TRY{Success?}

    TRY -->|DB Error| RETRY{Retryable?}
    TRY -->|LLM Error| LLM_RETRY{Retry Count?}
    TRY -->|External API Error| EXT_RETRY{Can Retry?}
    TRY -->|Success| SUCCESS[Return 200<br/>Success]

    RETRY -->|Yes| WAIT1[Exponential Backoff]
    RETRY -->|No| ERROR_500[Return 500<br/>Internal Error]

    LLM_RETRY -->|< Max| WAIT2[Backoff + Retry]
    LLM_RETRY -->|>= Max| ERROR_503[Return 503<br/>Service Unavailable]

    EXT_RETRY -->|Yes| WAIT3[Retry with Timeout]
    EXT_RETRY -->|No| ERROR_502[Return 502<br/>Bad Gateway]

    WAIT1 --> PROCESS
    WAIT2 --> PROCESS
    WAIT3 --> PROCESS

    ERROR_400 --> LOG[Log Error]
    ERROR_401 --> LOG
    ERROR_403 --> LOG
    ERROR_500 --> LOG
    ERROR_502 --> LOG
    ERROR_503 --> LOG

    LOG --> ALERT{Critical?}
    ALERT -->|Yes| NOTIFY[Send Alert]
    ALERT -->|No| METRIC[Update Metrics]

    NOTIFY --> METRIC
    METRIC --> END[End]
    SUCCESS --> END

    style SUCCESS fill:#99ff99
    style ERROR_400 fill:#ff9999
    style ERROR_401 fill:#ff9999
    style ERROR_403 fill:#ff9999
    style ERROR_500 fill:#ff9999
    style ERROR_502 fill:#ff9999
    style ERROR_503 fill:#ff9999
</div>

<hr />
<h2 id="12-class-diagrams">12. Class Diagrams</h2>
<h3 id="121-domain-model">12.1 Domain Model</h3>
<div class="mermaid">
classDiagram
    class Case {
        +str deployment_id
        +str tenant_id
        +str case_id
        +str case_detail
        +str iocs
        +List~Finding~ findings
        +to_dict() dict
        +check_non_empty_fields()
    }

    class Finding {
        +str source
        +str finding
    }

    class PlaybookTask {
        +str deployment_id
        +str tenant_id
        +str case_id
        +str playbook_run_id
        +dict task_details
        +to_dict() dict
    }

    Case "1" --> "*" Finding : contains
</div>

<h3 id="122-service-layer">12.2 Service Layer</h3>
<div class="mermaid">
classDiagram
    class OpenAICaseAnalysisService {
        -LLMWrapper model
        -SelfRepairJsonOutputParser parser
        -CaseAnalysis _case_analysis
        +run(case: Case) Tuple
    }

    class CaseAnalysis {
        -model
        -templates_factory
        -case2template
        -parser
        -playbook_task_repo
        +run(case: Case) Tuple
    }

    class ChatBotAgent {
        -Session session
        +stream() AsyncGenerator
        +extract_parameters() dict
    }

    OpenAICaseAnalysisService --> CaseAnalysis : uses
    CaseAnalysis --> PlaybookTaskRepo : queries
    ChatBotAgent --> PlaybookTaskRepo : queries
</div>

<h3 id="123-repository-layer">12.3 Repository Layer</h3>
<div class="mermaid">
classDiagram
    class PlaybookTaskRepo {
        -Session session
        -AIEmbedding embedding
        +add_task(task) PlaybookTaskModel
        +search_by_case() List
        +update_risk_score()
    }

    class CaseRepo {
        -Session session
        -AIEmbedding embedding
        +create_case() CaseModel
        +add_summary() CaseSummary
        +get_by_case_id() CaseModel
    }

    class ChatHistory {
        -Session session
        +add_thread() dict
        +store_chat_history() dict
        +get_chat_history() List
    }

    PlaybookTaskRepo --> PlaybookTaskModel : manages
    CaseRepo --> CaseModel : manages
    CaseRepo --> CaseSummary : manages
    ChatHistory --> Threads : manages
    ChatHistory --> ChatHistoryModel : manages
</div>

<h3 id="124-llm-integration">12.4 LLM Integration</h3>
<div class="mermaid">
classDiagram
    class LLMWrapper {
        -str backend
        -str model
        -float temperature
        -bool streaming
        -llm
        +invoke() Any
        +stream() Generator
        +astream() AsyncGenerator
        +with_structured_output()
    }

    class AIEmbedding {
        -str backend
        -str model
        +embed_text(text) List~float~
        +embed_batch(texts) List
    }

    class SelfRepairJsonOutputParser {
        +parse(text) dict
    }

    LLMWrapper --> ChatOpenAI : "backend=OPENAI"
    LLMWrapper --> ChatOllama : "backend=ONPREM"
    LLMWrapper --> SelfRepairJsonOutputParser : uses
</div>

<hr />
<h2 id="13-key-design-patterns">13. Key Design Patterns</h2>
<h3 id="131-repository-pattern">13.1 Repository Pattern</h3>
<ul>
<li><strong>Purpose</strong>: Abstract data access logic</li>
<li><strong>Implementation</strong>: Separate repository classes for each entity</li>
<li><strong>Benefits</strong>: Testability, decoupling, consistency</li>
</ul>
<h3 id="132-dependency-injection">13.2 Dependency Injection</h3>
<ul>
<li><strong>Purpose</strong>: Manage dependencies cleanly</li>
<li><strong>Implementation</strong>: FastAPI Depends(), constructor injection</li>
<li><strong>Benefits</strong>: Testability, flexibility, loose coupling</li>
</ul>
<h3 id="133-strategy-pattern">13.3 Strategy Pattern</h3>
<ul>
<li><strong>Purpose</strong>: Switch LLM backends dynamically</li>
<li><strong>Implementation</strong>: LLMWrapper with backend selection</li>
<li><strong>Benefits</strong>: Flexibility, extensibility</li>
</ul>
<h3 id="134-observer-pattern">13.4 Observer Pattern</h3>
<ul>
<li><strong>Purpose</strong>: Event-driven processing</li>
<li><strong>Implementation</strong>: Kafka pub/sub</li>
<li><strong>Benefits</strong>: Scalability, decoupling</li>
</ul>
<h3 id="135-factory-pattern">13.5 Factory Pattern</h3>
<ul>
<li><strong>Purpose</strong>: Create objects based on type</li>
<li><strong>Implementation</strong>: AnalysisTemplatesFactory</li>
<li><strong>Benefits</strong>: Centralized creation logic</li>
</ul>
<hr />
<h2 id="14-error-handling">14. Error Handling</h2>
<h3 id="141-exception-hierarchy">14.1 Exception Hierarchy</h3>
<pre class="highlight"><code class="language-python">class SiaException(Exception):
    &quot;&quot;&quot;Base exception&quot;&quot;&quot;
    pass

class InvalidInputException(SiaException):
    &quot;&quot;&quot;Invalid user input&quot;&quot;&quot;
    pass

class NoneResultException(SiaException):
    &quot;&quot;&quot;LLM returned None&quot;&quot;&quot;
    pass

class DatabaseException(SiaException):
    &quot;&quot;&quot;Database operation failed&quot;&quot;&quot;
    pass
</code></pre>

<h3 id="142-retry-logic">14.2 Retry Logic</h3>
<pre class="highlight"><code class="language-python">@retry((NoneResultException), tries=3, delay=2, backoff=2)
def run_analysis(case: Case):
    &quot;&quot;&quot;Retry on specific exceptions&quot;&quot;&quot;
    result = analyze(case)
    if result is None:
        raise NoneResultException()
    return result
</code></pre>

<hr />
<h2 id="15-performance-optimizations">15. Performance Optimizations</h2>
<h3 id="151-caching-strategy">15.1 Caching Strategy</h3>
<ul>
<li><strong>Redis</strong>: LLM response caching</li>
<li><strong>Hash-based keys</strong>: Deterministic cache hits</li>
<li><strong>TTL</strong>: Configurable expiration</li>
</ul>
<h3 id="152-vector-search-optimization">15.2 Vector Search Optimization</h3>
<ul>
<li><strong>HNSW Index</strong>: Fast approximate nearest neighbor</li>
<li><strong>Batch Processing</strong>: Group operations</li>
<li><strong>Selective Filtering</strong>: Reduce search space</li>
</ul>
<h3 id="153-connection-pooling">15.3 Connection Pooling</h3>
<ul>
<li><strong>SQLAlchemy</strong>: Automatic connection pooling</li>
<li><strong>Redis</strong>: Connection pool configuration</li>
<li><strong>Kafka</strong>: Reusable consumers</li>
</ul>
<hr />
<h2 id="15-testing-strategy">15. Testing Strategy</h2>
<h3 id="151-unit-testing">15.1 Unit Testing</h3>
<pre class="highlight"><code class="language-python"># tests/apis/test_case_service.py
import pytest
from siaservice.apis.services.cases import OpenAICaseAnalysisService
from siaservice.domain.soar.cases import Case

@pytest.fixture
def mock_playbook_repo(mocker):
    repo = mocker.Mock()
    repo.search_by_case.return_value = []
    return repo

@pytest.fixture
def case_service(mock_playbook_repo):
    return OpenAICaseAnalysisService(
        playbook_task_repo=mock_playbook_repo
    )

def test_case_analysis_success(case_service):
    case = Case(
        deployment_id=&quot;test&quot;,
        tenant_id=&quot;test&quot;,
        case_id=&quot;CASE-001&quot;,
        case_detail=&quot;Test case&quot;,
        iocs=&quot;192.168.1.1&quot;
    )

    result, is_cached = case_service.run(case)

    assert result is not None
    assert &quot;executive_summary&quot; in result
    assert &quot;conclusion&quot; in result
    assert isinstance(is_cached, bool)
</code></pre>

<h3 id="152-integration-testing">15.2 Integration Testing</h3>
<pre class="highlight"><code class="language-python"># tests/integration/test_case_flow.py
import pytest
from fastapi.testclient import TestClient
from main import app

@pytest.fixture
def client():
    return TestClient(app)

def test_case_analysis_flow(client):
    # Submit case
    response = client.post(
        &quot;/case/analysis&quot;,
        json={
            &quot;case&quot;: {
                &quot;deployment_id&quot;: &quot;test&quot;,
                &quot;tenant_id&quot;: &quot;test&quot;,
                &quot;case_id&quot;: &quot;CASE-001&quot;,
                &quot;case_detail&quot;: &quot;Test&quot;,
                &quot;iocs&quot;: &quot;192.168.1.1&quot;
            }
        }
    )
    assert response.status_code == 201
    task_id = response.json()[&quot;task_id&quot;]

    # Poll for result
    result_response = client.get(f&quot;/case/analysis/{task_id}&quot;)
    assert result_response.status_code == 200
</code></pre>

<h3 id="153-performance-testing">15.3 Performance Testing</h3>
<pre class="highlight"><code class="language-python"># tests/performance/test_vector_search.py
import time
import pytest
from siaservice.repositories.playbook_tasks_repo import PlaybookTaskRepo

def test_vector_search_performance(db_session, embedding_client):
    repo = PlaybookTaskRepo(db_session, embedding_client)

    start = time.time()
    results = repo.search_by_case(
        &quot;malicious IP&quot;,
        &quot;test-dep&quot;,
        &quot;test-tenant&quot;,
        &quot;CASE-001&quot;,
        limit=100
    )
    duration = time.time() - start

    assert duration &lt; 1.0  # Sub-second search
    assert len(results) &gt; 0
</code></pre>

<hr />
<h2 id="16-code-examples">16. Code Examples</h2>
<h3 id="161-adding-a-new-api-endpoint">16.1 Adding a New API Endpoint</h3>
<pre class="highlight"><code class="language-python"># siaservice/routers/example.py
from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session
from siaservice.db.pgv.db import get_db_for_tenant
from siaservice.apis.services.base.models import ExamplePayload
from siaservice.common import get_logger

logger = get_logger(__name__)
router = APIRouter(prefix=&quot;/example&quot;, tags=[&quot;Example&quot;])

@router.post(&quot;/process&quot;, status_code=201)
def process_example(
    payload: ExamplePayload,
    db: Session = Depends(get_db_for_tenant)
):
    logger.info(f&quot;Processing example: {payload.id}&quot;)

    # Business logic here
    result = {&quot;status&quot;: &quot;success&quot;}

    return result
</code></pre>

<h3 id="162-creating-a-new-repository">16.2 Creating a New Repository</h3>
<pre class="highlight"><code class="language-python"># siaservice/repositories/example_repo.py
from sqlalchemy.orm import Session
from siaservice.db.pgv.models import ExampleModel
from siaservice.db.embed.embedding import AIEmbedding
from typing import List, Optional

class ExampleRepo:
    def __init__(self, session: Session, embedding: AIEmbedding):
        self.session = session
        self.embedding = embedding

    def create(self, data: dict) -&gt; ExampleModel:
        &quot;&quot;&quot;Create new record&quot;&quot;&quot;
        # Generate embedding if needed
        text = data.get(&quot;description&quot;, &quot;&quot;)
        embedding_vector = self.embedding.embed_text(text)

        model = ExampleModel(
            **data,
            embedding=embedding_vector
        )

        self.session.add(model)
        self.session.commit()
        self.session.refresh(model)

        return model

    def search_similar(
        self,
        query: str,
        tenant_id: str,
        limit: int = 10
    ) -&gt; List[ExampleModel]:
        &quot;&quot;&quot;Vector similarity search&quot;&quot;&quot;
        query_vector = self.embedding.embed_text(query)

        results = self.session.query(ExampleModel).filter(
            ExampleModel.tenant_id == tenant_id
        ).order_by(
            ExampleModel.embedding.l2_distance(query_vector)
        ).limit(limit).all()

        return results
</code></pre>

<h3 id="163-adding-a-new-celery-task">16.3 Adding a New Celery Task</h3>
<pre class="highlight"><code class="language-python"># worker.py - Add new task
from celery import Celery
from siaservice.db.pgv.db import db_session
from siaservice.common import get_logger

logger = get_logger(__name__)
celery = Celery(__name__)

@celery.task(name=&quot;process_example_task&quot;)
def process_example_task(data: dict):
    &quot;&quot;&quot;
    Background task for processing examples
    &quot;&quot;&quot;
    tenant_id = data[&quot;tenant_id&quot;]
    logger.info(f&quot;Processing example for tenant: {tenant_id}&quot;)

    with db_session(tenant_id) as session:
        # Your logic here
        result = perform_processing(session, data)

        return {&quot;result&quot;: result}

def perform_processing(session, data):
    # Implementation
    return {&quot;status&quot;: &quot;completed&quot;}
</code></pre>

<h3 id="164-adding-a-new-llm-prompt-template">16.4 Adding a New LLM Prompt Template</h3>
<pre class="highlight"><code class="language-python"># siaservice/analysis/templates.py
class AnalysisTemplatesFactory:
    def get_example_template(self) -&gt; str:
        &quot;&quot;&quot;
        Returns prompt template for example analysis
        &quot;&quot;&quot;
        return &quot;&quot;&quot;
You are an expert security analyst. Analyze the following data:

Context:
{context}

Data to analyze:
{data}

Provide your analysis in JSON format:
{{
  &quot;summary&quot;: &quot;...&quot;,
  &quot;findings&quot;: [...],
  &quot;recommendations&quot;: [...]
}}
&quot;&quot;&quot;

    def get_by_type(self, analysis_type: str) -&gt; str:
        &quot;&quot;&quot;Get template by type&quot;&quot;&quot;
        templates = {
            &quot;case&quot;: self.get_by_case,
            &quot;example&quot;: self.get_example_template,
        }

        return templates.get(analysis_type, self.get_default)()
</code></pre>

<h3 id="165-database-migration-example">16.5 Database Migration Example</h3>
<pre class="highlight"><code class="language-python"># migrations/versions/xxxxx_add_example_table.py
&quot;&quot;&quot;Add example table

Revision ID: xxxxx
Revises: yyyyy
Create Date: 2025-11-11 16:00:00.000000

&quot;&quot;&quot;
from alembic import op
import sqlalchemy as sa
from pgvector.sqlalchemy import Vector

# revision identifiers
revision = 'xxxxx'
down_revision = 'yyyyy'
branch_labels = None
depends_on = None

def upgrade():
    op.create_table(
        'examples',
        sa.Column('id', sa.UUID(), nullable=False),
        sa.Column('tenant_id', sa.String(256), nullable=False),
        sa.Column('name', sa.String(255), nullable=False),
        sa.Column('description', sa.Text(), nullable=True),
        sa.Column('embedding', Vector(1536), nullable=True),
        sa.Column('created_at', sa.DateTime(timezone=True), 
                  server_default=sa.text('now()'), nullable=False),
        sa.PrimaryKeyConstraint('id')
    )

    op.create_index(
        'ix_examples_tenant_id',
        'examples',
        ['tenant_id']
    )

    op.create_index(
        'ix_examples_embedding',
        'examples',
        ['embedding'],
        postgresql_using='hnsw',
        postgresql_with={'m': 16, 'ef_construction': 64},
        postgresql_ops={'embedding': 'vector_l2_ops'}
    )

def downgrade():
    op.drop_index('ix_examples_embedding', table_name='examples')
    op.drop_index('ix_examples_tenant_id', table_name='examples')
    op.drop_table('examples')
</code></pre>

<h3 id="166-adding-environment-configuration">16.6 Adding Environment Configuration</h3>
<pre class="highlight"><code class="language-python"># .env additions
# Example Service Configuration
EXAMPLE_API_KEY=your_api_key_here
EXAMPLE_BASE_URL=https://api.example.com
EXAMPLE_TIMEOUT=30
EXAMPLE_MAX_RETRIES=3
</code></pre>

<pre class="highlight"><code class="language-python"># siaservice/services/example_client.py
import os
import requests
from typing import Dict, Any
from siaservice.common import get_logger

logger = get_logger(__name__)

class ExampleClient:
    def __init__(self):
        self.api_key = os.getenv(&quot;EXAMPLE_API_KEY&quot;)
        self.base_url = os.getenv(&quot;EXAMPLE_BASE_URL&quot;)
        self.timeout = int(os.getenv(&quot;EXAMPLE_TIMEOUT&quot;, 30))
        self.max_retries = int(os.getenv(&quot;EXAMPLE_MAX_RETRIES&quot;, 3))

    def call_api(self, endpoint: str, data: Dict[str, Any]) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;Make API call with retry logic&quot;&quot;&quot;
        url = f&quot;{self.base_url}/{endpoint}&quot;
        headers = {&quot;Authorization&quot;: f&quot;Bearer {self.api_key}&quot;}

        for attempt in range(self.max_retries):
            try:
                response = requests.post(
                    url,
                    json=data,
                    headers=headers,
                    timeout=self.timeout
                )
                response.raise_for_status()
                return response.json()

            except requests.RequestException as e:
                logger.warning(f&quot;API call failed (attempt {attempt + 1}): {e}&quot;)
                if attempt == self.max_retries - 1:
                    raise

        return {}
</code></pre>

<hr />
<p><strong>Document Version</strong>: 1.0<br />
<strong>Last Updated</strong>: November 11, 2025<br />
<strong>Status</strong>: Active Development</p>
    </main>

    <footer class="footer">
        <p>&copy; 2025 Securaa Security Platform. All rights reserved.</p>
        <p>Documentation generated on December 04, 2025</p>
    </footer>

    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <script>
        mermaid.initialize({
            startOnLoad: true,
            theme: 'base',
            themeVariables: {
                primaryColor: '#4f46e5',
                primaryTextColor: '#1f2937',
                primaryBorderColor: '#818cf8',
                lineColor: '#6b7280',
                secondaryColor: '#f3f4f6',
                tertiaryColor: '#e5e7eb',
                background: '#ffffff',
                mainBkg: '#f9fafb',
                secondBkg: '#f3f4f6',
                border1: '#e5e7eb',
                border2: '#d1d5db',
                fontFamily: 'Inter, sans-serif',
                fontSize: '14px',
                nodeBorder: '#4f46e5',
                clusterBkg: '#f0f4f8',
                clusterBorder: '#818cf8',
                edgeLabelBackground: '#ffffff'
            },
            flowchart: {
                htmlLabels: true,
                useMaxWidth: true,
                curve: 'basis',
                padding: 15,
                nodeSpacing: 50,
                rankSpacing: 50
            },
            sequence: {
                actorMargin: 50,
                width: 150,
                height: 65,
                boxMargin: 10,
                boxTextMargin: 5,
                noteMargin: 10,
                messageMargin: 35,
                mirrorActors: true,
                useMaxWidth: true
            },
            er: {
                useMaxWidth: true,
                entityPadding: 15,
                fontSize: 12
            },
            class: {
                useMaxWidth: true,
                padding: 10
            },
            gantt: {
                useMaxWidth: true,
                barHeight: 20,
                barGap: 4,
                topPadding: 50,
                leftPadding: 75
            },
            pie: {
                useMaxWidth: true,
                textPosition: 0.5
            },
            mindmap: {
                useMaxWidth: true,
                padding: 10
            },
            securityLevel: 'loose',
            logLevel: 'error'
        });

        // Re-render mermaid diagrams after page load for better sizing
        window.addEventListener('load', function() {
            setTimeout(function() {
                document.querySelectorAll('.mermaid').forEach(function(el) {
                    if (el.getAttribute('data-processed') !== 'true') {
                        mermaid.init(undefined, el);
                    }
                });
            }, 100);
        });
    </script>
</body>
</html>
